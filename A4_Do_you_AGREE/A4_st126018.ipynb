{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab31888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\A4_AIT\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "from random import randrange, randint, shuffle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951de5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc67c5",
   "metadata": {},
   "source": [
    "Task 1. Training BERT from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85103819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset rows: 100000\n",
      "columns: ['text']\n",
      "len(sentences): 100000\n",
      "preview: \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train[:100000]\")\n",
    "\n",
    "print(\"dataset rows:\", dataset.num_rows)\n",
    "print(\"columns:\", dataset.column_names)\n",
    "\n",
    "sentences = dataset[\"text\"]\n",
    "print(\"len(sentences):\", len(sentences))\n",
    "print(\"preview:\", sentences[0][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fc0c5",
   "metadata": {},
   "source": [
    "preview: is empty because WikiText contains a lot of empty lines at the start and throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193357a",
   "metadata": {},
   "source": [
    "Find a non-empty preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c37bfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-empty preview:  = Valkyria Chronicles III = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show first non-empty line\n",
    "for s in sentences[:200]:\n",
    "    if s and s.strip():\n",
    "        print(\"non-empty preview:\", s[:200])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91678c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 62418\n",
      "example: = valkyria chronicles iii =\n"
     ]
    }
   ],
   "source": [
    "#Do not want aggressive punctuation removal here, because WikiText has markup like = Heading =.\n",
    "import re\n",
    "\n",
    "text = []\n",
    "for s in sentences:\n",
    "    if not s or not s.strip():\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # keep lines with at least 5 words (helps reduce junk lines)\n",
    "    if len(s.split()) >= 5:\n",
    "        text.append(s)\n",
    "\n",
    "print(\"After cleaning:\", len(text))\n",
    "print(\"example:\", text[0][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d421493",
   "metadata": {},
   "source": [
    "Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00d386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "Top tokens: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a', '=', '\"', 'was', '@-@', 'on', 'as', 'that', \"'s\", 'for', 'with', 'by', ')', '(', 'is', 'he', 'his', 'at']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for s in text:\n",
    "    counter.update(s.split())\n",
    "\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "most_common = [w for w, _ in counter.most_common(VOCAB_SIZE - len(special_tokens))]\n",
    "vocab = special_tokens + most_common\n",
    "\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "\n",
    "print(\"Vocab size:\", len(word2id))\n",
    "print(\"Top tokens:\", vocab[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9648fb5",
   "metadata": {},
   "source": [
    "Create token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f7ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_list size: 62418\n",
      "example ids: [13, 7960, 6351, 1213, 13]\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "unk_id = word2id[\"[UNK]\"]\n",
    "\n",
    "for s in text:\n",
    "    token_list.append([word2id.get(w, unk_id) for w in s.split()])\n",
    "\n",
    "print(\"token_list size:\", len(token_list))\n",
    "print(\"example ids:\", token_list[0][:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "batch_size = 8\n",
    "max_mask   = 10 \n",
    "max_len    = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange, randint, shuffle, random\n",
    "\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "\n",
    "    while positive != batch_size // 2 or negative != batch_size // 2:\n",
    "        # choose two sentences\n",
    "        a_idx, b_idx = randrange(len(token_list)), randrange(len(token_list))\n",
    "        tokens_a, tokens_b = token_list[a_idx], token_list[b_idx]\n",
    "\n",
    "        max_tokens_total = max_len - 3\n",
    "        if max_tokens_total <= 0:\n",
    "            raise ValueError(\"max_len too small\")\n",
    "\n",
    "        # split budget roughly half-half\n",
    "        max_a = max_tokens_total // 2\n",
    "        max_b = max_tokens_total - max_a\n",
    "\n",
    "        tokens_a = tokens_a[:max_a]\n",
    "        tokens_b = tokens_b[:max_b]\n",
    "\n",
    "        # 1) input ids with special tokens\n",
    "        input_ids = [word2id[\"[CLS]\"]] + tokens_a + [word2id[\"[SEP]\"]] + tokens_b + [word2id[\"[SEP]\"]]\n",
    "\n",
    "        # 2) segment ids\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # 3) MLM masking\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "\n",
    "        cand_masked_pos = [i for i, tok in enumerate(input_ids)\n",
    "                           if tok != word2id[\"[CLS]\"] and tok != word2id[\"[SEP]\"]]\n",
    "        shuffle(cand_masked_pos)\n",
    "\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "\n",
    "            r = random()\n",
    "            if r < 0.1:  # 10% random token\n",
    "                rand_id = randint(0, len(word2id) - 1)\n",
    "                input_ids[pos] = rand_id\n",
    "            elif r < 0.9:  # 80% [MASK]\n",
    "                input_ids[pos] = word2id[\"[MASK]\"]\n",
    "            else:  # 10% keep original\n",
    "                pass\n",
    "\n",
    "        # pad input_ids/segment_ids\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([word2id[\"[PAD]\"]] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # pad masked arrays\n",
    "        if max_mask > n_pred:\n",
    "            pad_m = max_mask - n_pred\n",
    "            masked_tokens.extend([word2id[\"[PAD]\"]] * pad_m)\n",
    "            masked_pos.extend([0] * pad_m)\n",
    "\n",
    "        # NSP label \n",
    "        if a_idx + 1 == b_idx and positive < batch_size // 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif a_idx + 1 != b_idx and negative < batch_size // 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bd34a",
   "metadata": {},
   "source": [
    "Test one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa4303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([8, 64])\n",
      "segment_ids: torch.Size([8, 64])\n",
      "masked_tokens: torch.Size([8, 10])\n",
      "masked_pos: torch.Size([8, 10])\n",
      "isNext: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "print(\"input_ids:\", input_ids.shape)\n",
    "print(\"segment_ids:\", segment_ids.shape)\n",
    "print(\"masked_tokens:\", masked_tokens.shape)\n",
    "print(\"masked_pos:\", masked_pos.shape)\n",
    "print(\"isNext:\", isNext.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd7115",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_k: 64 d_v: 64\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2 \n",
    "n_heads  = 2 \n",
    "d_model  = 128 \n",
    "d_ff     = d_model * 4\n",
    "d_k = d_v = d_model // n_heads\n",
    "n_segments = 2 \n",
    "\n",
    "print(\"d_k:\", d_k, \"d_v:\", d_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdab0de",
   "metadata": {},
   "source": [
    "Embedding (Token + Position + Segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167ed1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, n_segments):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)      # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)         # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)      # segment embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        # x: (bs, max_len)\n",
    "        bs, seq_len = x.size()\n",
    "        pos = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(bs, seq_len)\n",
    "        out = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d038f",
   "metadata": {},
   "source": [
    "Attention mask (pad mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94036dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, pad_id=0):\n",
    "    # seq_q: (bs, len_q), seq_k: (bs, len_k)\n",
    "    bs, len_q = seq_q.size()\n",
    "    bs, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.eq(pad_id).unsqueeze(1)  # (bs, 1, len_k)\n",
    "    return pad_attn_mask.expand(bs, len_q, len_k)  # (bs, len_q, len_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49891a3",
   "metadata": {},
   "source": [
    "Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d0294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # Q,K,V: (bs, heads, len, d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(Q.size(-1))  # (bs, heads, len_q, len_k)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf1c7e",
   "metadata": {},
   "source": [
    "Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d2233f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, n_heads * d_k)\n",
    "        self.W_K = nn.Linear(d_model, n_heads * d_k)\n",
    "        self.W_V = nn.Linear(d_model, n_heads * d_v)\n",
    "        self.fc  = nn.Linear(n_heads * d_v, d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # Q,K,V: (bs, len, d_model)\n",
    "        residual = Q\n",
    "        bs = Q.size(0)\n",
    "\n",
    "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)  # (bs, heads, len, d_k)\n",
    "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        # attn_mask: (bs, len_q, len_k) -> (bs, heads, len_q, len_k)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "\n",
    "        context, attn = self.attention(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v)\n",
    "        output = self.fc(context)\n",
    "\n",
    "        return self.norm(output + residual), attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f5b0b",
   "metadata": {},
   "source": [
    "FeedForward + EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6368116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fc2(F.gelu(self.fc1(x)))\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, d_v)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261fad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding out: torch.Size([8, 64, 128])\n",
      "mask: torch.Size([8, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# quick forward shape check ( just embedding + mask)\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "\n",
    "emb = Embedding(vocab_size=len(word2id), d_model=d_model, max_len=max_len, n_segments=n_segments).to(device)\n",
    "x = emb(input_ids, segment_ids)\n",
    "mask = get_attn_pad_mask(input_ids, input_ids, pad_id=word2id[\"[PAD]\"])\n",
    "\n",
    "print(\"embedding out:\", x.shape)\n",
    "print(\"mask:\", mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f3ecb",
   "metadata": {},
   "source": [
    "BERT model (Encoder stack + MLM + NSP heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, d_v, n_segments,\n",
    "                 vocab_size, max_len, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, d_model, max_len, n_segments)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(n_heads, d_model, d_ff, d_k, d_v)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        #NSP head \n",
    "        self.fc_nsp = nn.Linear(d_model, 2)\n",
    "\n",
    "        #MLM head \n",
    "        self.fc_mlm1 = nn.Linear(d_model, d_model)\n",
    "        self.act = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc_mlm2 = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        # input_ids: (bs, max_len)\n",
    "        # segment_ids: (bs, max_len)\n",
    "        # masked_pos: (bs, max_mask)\n",
    "\n",
    "        output = self.embedding(input_ids, segment_ids)  # (bs, max_len, d_model)\n",
    "\n",
    "        enc_self_attn_mask = get_attn_pad_mask(\n",
    "            input_ids, input_ids, pad_id=word2id[\"[PAD]\"]\n",
    "        )  # (bs, max_len, max_len)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output, _ = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        # NSP: use [CLS] token (position 0)\n",
    "        cls_output = output[:, 0]                 # (bs, d_model)\n",
    "        logits_nsp = self.fc_nsp(cls_output)      # (bs, 2)\n",
    "\n",
    "        #masked positions\n",
    "        bs, max_m = masked_pos.size()\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(bs, max_m, output.size(-1))\n",
    "        h_masked = torch.gather(output, 1, masked_pos)  # (bs, max_mask, d_model)\n",
    "\n",
    "        h_masked = self.fc_mlm1(h_masked)\n",
    "        h_masked = self.act(h_masked)\n",
    "        h_masked = self.norm(h_masked)\n",
    "        logits_lm = self.fc_mlm2(h_masked)  # (bs, max_mask, vocab_size)\n",
    "\n",
    "        return logits_lm, logits_nsp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4be66f",
   "metadata": {},
   "source": [
    "Instantiate model + quick forward test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e1b07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)\n",
    "\n",
    "model = BERT(\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_model=d_model,\n",
    "    d_ff=d_ff,\n",
    "    d_k=d_k,\n",
    "    d_v=d_v,\n",
    "    n_segments=n_segments,\n",
    "    vocab_size=vocab_size,\n",
    "    max_len=max_len,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2id[\"[PAD]\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8b9cb",
   "metadata": {},
   "source": [
    "Training (MLM + NSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss + optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2id[\"[PAD]\"])# ignore PAD in MLM labels\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341939b",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 50/300 [01:55<13:05,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50/300 | avg loss: 10.7784 | lm: 9.6458 | nsp: 0.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 100/300 [03:49<08:21,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100/300 | avg loss: 10.0728 | lm: 8.9756 | nsp: 0.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 150/300 [05:35<03:33,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 150/300 | avg loss: 9.3931 | lm: 8.5256 | nsp: 0.6776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 200/300 [07:18<03:17,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200/300 | avg loss: 8.8667 | lm: 8.0579 | nsp: 0.6905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 250/300 [08:56<02:20,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 250/300 | avg loss: 8.3771 | lm: 7.3926 | nsp: 0.6865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [10:50<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300/300 | avg loss: 8.1841 | lm: 7.4469 | nsp: 0.6943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 300\n",
    "print_every = 50\n",
    "\n",
    "model.train()\n",
    "total_loss = 0.0\n",
    "\n",
    "for step in tqdm(range(1, num_steps + 1), desc=\"Training\"):\n",
    "    batch = make_batch()\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    masked_tokens = masked_tokens.to(device)\n",
    "    masked_pos = masked_pos.to(device)\n",
    "    isNext = isNext.to(device).long()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)\n",
    "    loss_nsp = nn.CrossEntropyLoss()(logits_nsp, isNext)\n",
    "\n",
    "    loss = loss_lm + loss_nsp\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    if step % print_every == 0:\n",
    "        avg = total_loss / print_every\n",
    "        print(f\"step {step}/{num_steps} | avg loss: {avg:.4f} | lm: {loss_lm.item():.4f} | nsp: {loss_nsp.item():.4f}\")\n",
    "        total_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8776fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: task1_bert_scratch.pth\n"
     ]
    }
   ],
   "source": [
    "save_path = \"task1_bert_scratch.pth\"\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"word2id\": word2id,\n",
    "    \"id2word\": id2word,\n",
    "    \"config\": {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_heads\": n_heads,\n",
    "        \"d_model\": d_model,\n",
    "        \"d_ff\": d_ff,\n",
    "        \"d_k\": d_k,\n",
    "        \"d_v\": d_v,\n",
    "        \"n_segments\": n_segments,\n",
    "        \"max_len\": max_len,\n",
    "        \"max_mask\": max_mask,\n",
    "        \"vocab_size\": len(word2id),\n",
    "    }\n",
    "}, save_path)\n",
    "\n",
    "print(\" Saved:\", save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddb864",
   "metadata": {},
   "source": [
    "MLM inference check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4e84c",
   "metadata": {},
   "source": [
    "helper to predict a masked token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a831ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, word2id, max_len):\n",
    "    sentence = sentence.lower().strip()\n",
    "    words = sentence.split()\n",
    "    ids = [word2id[\"[CLS]\"]] + [word2id.get(w, word2id[\"[UNK]\"]) for w in words] + [word2id[\"[SEP]\"]]\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "        ids[-1] = word2id[\"[SEP]\"]\n",
    "    pad_len = max_len - len(ids)\n",
    "    ids += [word2id[\"[PAD]\"]] * pad_len\n",
    "    seg = [0] * max_len\n",
    "    return ids, seg\n",
    "\n",
    "def predict_mask(model, sentence, mask_index, topk=10):\n",
    "    model.eval()\n",
    "    ids, seg = encode_sentence(sentence, word2id, max_len)\n",
    "    ids = torch.LongTensor([ids]).to(device)\n",
    "    seg = torch.LongTensor([seg]).to(device)\n",
    "\n",
    "    # set one position to [MASK]\n",
    "    ids[0, mask_index] = word2id[\"[MASK]\"]\n",
    "    masked_pos = torch.LongTensor([[mask_index] + [0]*(max_mask-1)]).to(device)  # shape (1, max_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_lm, _ = model(ids, seg, masked_pos)  # (1, max_mask, vocab)\n",
    "        scores = logits_lm[0, 0]                    # first masked position\n",
    "\n",
    "    top_ids = torch.topk(scores, k=topk).indices.tolist()\n",
    "    return [id2word[i] for i in top_ids]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b388d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictions: ['=', ',', 'the', 'in', '[UNK]', '.', 'to', 'and', 'a', 'of']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"the man is playing the guitar on the stage\"\n",
    "# mask_index counts in the tokenized input including [CLS] at position 0\n",
    "# e.g., position 3 likely corresponds to \"is\" depending on splitting\n",
    "print(\"Top predictions:\", predict_mask(model, test_sentence, mask_index=3, topk=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68472d0",
   "metadata": {},
   "source": [
    "WikiText has many headings like = something = so = becomes very frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f96d68",
   "metadata": {},
   "source": [
    "Task 2. Sentence Embedding with Sentence BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c787b",
   "metadata": {},
   "source": [
    "Load Task-1 checkpoint (word2id + config + weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80470a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab: 30000\n",
      "d_model: 128 max_len: 64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"task1_bert_scratch.pth\", map_location=\"cpu\")\n",
    "word2id = ckpt[\"word2id\"]\n",
    "id2word = ckpt[\"id2word\"]\n",
    "cfg = ckpt[\"config\"]\n",
    "\n",
    "print(\"Loaded vocab:\", cfg[\"vocab_size\"])\n",
    "print(\"d_model:\", cfg[\"d_model\"], \"max_len:\", cfg[\"max_len\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c262599",
   "metadata": {},
   "source": [
    "Rebuild the same BERT + load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e7ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "✅ Loaded Task-1 weights into BERT\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "model = BERT(\n",
    "    n_layers=cfg[\"n_layers\"],\n",
    "    n_heads=cfg[\"n_heads\"],\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    d_ff=cfg[\"d_ff\"],\n",
    "    d_k=cfg[\"d_k\"],\n",
    "    d_v=cfg[\"d_v\"],\n",
    "    n_segments=cfg[\"n_segments\"],\n",
    "    vocab_size=cfg[\"vocab_size\"],\n",
    "    max_len=cfg[\"max_len\"],\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
    "print(\"Loaded Task-1 weights into BERT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd1b90",
   "metadata": {},
   "source": [
    "Load SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b210c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\A4_AIT\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--snli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 1428529.00 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 2220030.70 examples/s]\n",
      "Generating train split: 100%|██████████| 550152/550152 [00:00<00:00, 5092738.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 550152\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 10000/10000 [00:00<00:00, 446406.76 examples/s]\n",
      "Filter: 100%|██████████| 10000/10000 [00:00<00:00, 444443.69 examples/s]\n",
      "Filter: 100%|██████████| 550152/550152 [00:00<00:00, 605669.56 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [0 1 2]\n",
      "train size: 549367 val size: 9842 test size: 9824\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "snli = load_dataset(\"snli\")\n",
    "print(snli)\n",
    "\n",
    "# filter out label == -1 \n",
    "snli = snli.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "print(\"labels:\", np.unique(snli[\"train\"][\"label\"]))\n",
    "print(\"train size:\", len(snli[\"train\"]), \"val size:\", len(snli[\"validation\"]), \"test size:\", len(snli[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34741b01",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dc32f",
   "metadata": {},
   "source": [
    "text → ids + attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33487377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PAD_ID = word2id[\"[PAD]\"]\n",
    "UNK_ID = word2id[\"[UNK]\"]\n",
    "CLS_ID = word2id[\"[CLS]\"]\n",
    "SEP_ID = word2id[\"[SEP]\"]\n",
    "\n",
    "max_len = cfg[\"max_len\"]\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def encode_pair(premise: str, hypothesis: str):\n",
    "    # normalize\n",
    "    p = normalize_text(premise)\n",
    "    h = normalize_text(hypothesis)\n",
    "\n",
    "    # word-level tokens\n",
    "    p_ids = [word2id.get(w, UNK_ID) for w in p.split()]\n",
    "    h_ids = [word2id.get(w, UNK_ID) for w in h.split()]\n",
    "\n",
    "    # build CLS premise SEP hypothesis SEP\n",
    "    ids = [CLS_ID] + p_ids + [SEP_ID] + h_ids + [SEP_ID]\n",
    "\n",
    "    # truncate to max_len (keep last token as SEP)\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "        ids[-1] = SEP_ID\n",
    "\n",
    "    attn = [1] * len(ids)\n",
    "\n",
    "    # pad\n",
    "    pad_len = max_len - len(ids)\n",
    "    if pad_len > 0:\n",
    "        ids += [PAD_ID] * pad_len\n",
    "        attn += [0] * pad_len\n",
    "\n",
    "    return ids, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8536e9b",
   "metadata": {},
   "source": [
    "preprocess_function for HuggingFace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1bb5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for p, h in zip(examples[\"premise\"], examples[\"hypothesis\"]):\n",
    "        ids, attn = encode_pair(p, h)\n",
    "        input_ids.append(ids)\n",
    "        attention_mask.append(attn)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": examples[\"label\"],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b818c",
   "metadata": {},
   "source": [
    "apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d25baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20000/20000 [00:00<00:00, 50235.28 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 52631.10 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 41657.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'input_ids', 'attention_mask', 'labels'])\n",
      "train rows: 20000 val rows: 2000 test rows: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_subset = snli[\"train\"].shuffle(seed=SEED).select(range(20000))\n",
    "val_subset   = snli[\"validation\"].select(range(2000))\n",
    "test_subset  = snli[\"test\"].select(range(2000)) \n",
    "\n",
    "tokenized_train = train_subset.map(preprocess_function, batched=True, remove_columns=[\"premise\",\"hypothesis\"])\n",
    "tokenized_val   = val_subset.map(preprocess_function, batched=True, remove_columns=[\"premise\",\"hypothesis\"])\n",
    "tokenized_test  = test_subset.map(preprocess_function, batched=True, remove_columns=[\"premise\",\"hypothesis\"])\n",
    "\n",
    "print(tokenized_train[0].keys())\n",
    "print(\"train rows:\", len(tokenized_train), \"val rows:\", len(tokenized_val), \"test rows:\", len(tokenized_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9089842",
   "metadata": {},
   "source": [
    "quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edb66b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids len: 64\n",
      "attention_mask len: 64\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "ex = tokenized_train[0]\n",
    "print(\"input_ids len:\", len(ex[\"input_ids\"]))\n",
    "print(\"attention_mask len:\", len(ex[\"attention_mask\"]))\n",
    "print(\"label:\", ex[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af85b7",
   "metadata": {},
   "source": [
    "create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2c84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64]) torch.Size([32, 64]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size_nli = 32  \n",
    "\n",
    "train_dataloader = DataLoader(tokenized_train, batch_size=batch_size_nli, shuffle=True)\n",
    "eval_dataloader  = DataLoader(tokenized_val, batch_size=batch_size_nli)\n",
    "test_dataloader  = DataLoader(tokenized_test, batch_size=batch_size_nli)\n",
    "\n",
    "# quick shape check\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch[\"input_ids\"].shape, batch[\"attention_mask\"].shape, batch[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303b16c",
   "metadata": {},
   "source": [
    "Mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebeb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_embeds: (bs, seq_len, hidden)\n",
    "# attention_mask: (bs, seq_len)\n",
    "\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "    pooled = torch.sum(token_embeds * in_mask, dim=1) / torch.clamp(in_mask.sum(dim=1), min=1e-9)\n",
    "    return pooled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c1738",
   "metadata": {},
   "source": [
    "Siamese forward utils (u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f86e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_ID = word2id[\"[SEP]\"]\n",
    "\n",
    "def split_pair(input_ids, attention_mask):\n",
    "    # input_ids: (bs, max_len)\n",
    "    # find first SEP position in each row\n",
    "    bs, L = input_ids.shape\n",
    "    sep_pos = (input_ids == SEP_ID).int().argmax(dim=1)  # (bs,)\n",
    "\n",
    "    # build masks for A/B\n",
    "    idxs = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(bs, L)\n",
    "\n",
    "    mask_a = (idxs <= sep_pos.unsqueeze(1)).long()\n",
    "    mask_b = (idxs >= sep_pos.unsqueeze(1)).long()\n",
    "\n",
    "    ids_a = input_ids\n",
    "    ids_b = input_ids\n",
    "\n",
    "    attn_a = attention_mask * mask_a\n",
    "    attn_b = attention_mask * mask_b\n",
    "\n",
    "    return ids_a, attn_a, ids_b, attn_b\n",
    "\n",
    "def encode_sentence_embedding(model, input_ids, attention_mask, segment_ids):\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9938384",
   "metadata": {},
   "source": [
    "Add encode() method to BERT (returns last hidden state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6105502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Added BERT.encode() to return hidden states\n"
     ]
    }
   ],
   "source": [
    "def bert_encode(self, input_ids, segment_ids):\n",
    "    output = self.embedding(input_ids, segment_ids)\n",
    "    enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, pad_id=word2id[\"[PAD]\"])\n",
    "    for layer in self.layers:\n",
    "        output, _ = layer(output, enc_self_attn_mask)\n",
    "    return output  # (bs, max_len, d_model)\n",
    "\n",
    "# monkey-patch method into BERT instance/class\n",
    "BERT.encode = bert_encode\n",
    "print(\" Added BERT.encode() to return hidden states\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b5849",
   "metadata": {},
   "source": [
    "Compute u,v embeddings for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_uv_embeddings(model, input_ids, attention_mask):\n",
    "    segment_ids = torch.zeros_like(input_ids) \n",
    "    hidden = model.encode(input_ids, segment_ids)  # (bs, L, d_model)\n",
    "\n",
    "    ids_a, attn_a, ids_b, attn_b = split_pair(input_ids, attention_mask)\n",
    "\n",
    "    u = mean_pool(hidden, attn_a)\n",
    "    v = mean_pool(hidden, attn_b)\n",
    "    return u, v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e328a",
   "metadata": {},
   "source": [
    "SoftmaxLoss head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35515cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_labels=3):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model * 3, num_labels)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        uv_abs = torch.abs(u - v)\n",
    "        x = torch.cat([u, v, uv_abs], dim=-1)\n",
    "        return self.linear(x)\n",
    "\n",
    "classifier_head = SoftmaxClassifier(cfg[\"d_model\"], num_labels=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_bert = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_cls  = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c457e70",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca076626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 625/625 [00:18<00:00, 33.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=1.0945, acc=0.3735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_epochs = 1 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    classifier_head.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    preds_all, labels_all = [], []\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer_bert.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "\n",
    "        u, v = batch_uv_embeddings(model, input_ids, attention_mask)\n",
    "        logits = classifier_head(u, v)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_bert.step()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        preds_all.extend(preds.tolist())\n",
    "        labels_all.extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "    acc = accuracy_score(labels_all, preds_all)\n",
    "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dataloader):.4f}, acc={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604583d9",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42ac1967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 63/63 [00:00<00:00, 144.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.44      0.44      0.44       663\n",
      "      neutral       0.39      0.63      0.48       677\n",
      "contradiction       0.35      0.14      0.20       660\n",
      "\n",
      "     accuracy                           0.41      2000\n",
      "    macro avg       0.40      0.40      0.38      2000\n",
      " weighted avg       0.40      0.41      0.38      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "classifier_head.eval()\n",
    "\n",
    "preds_all, labels_all = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Validation\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        u, v = batch_uv_embeddings(model, input_ids, attention_mask)\n",
    "        logits = classifier_head(u, v)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        preds_all.extend(preds.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "print(classification_report(labels_all, preds_all, target_names=[\"entailment\",\"neutral\",\"contradiction\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67089b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved task2_sbert_snli_softmaxloss.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"bert_state_dict\": model.state_dict(),\n",
    "    \"classifier_state_dict\": classifier_head.state_dict(),\n",
    "    \"word2id\": word2id,\n",
    "    \"id2word\": id2word,\n",
    "    \"cfg\": cfg\n",
    "}, \"task2_sbert_snli_softmaxloss.pth\")\n",
    "\n",
    "print(\"Saved task2_sbert_snli_softmaxloss.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e12668",
   "metadata": {},
   "source": [
    "Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac9e4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "def predict_nli(premise, hypothesis):\n",
    "    model.eval()\n",
    "    classifier_head.eval()\n",
    "\n",
    "    ids, attn = encode_pair(premise, hypothesis)\n",
    "    input_ids = torch.LongTensor([ids]).to(device)\n",
    "    attention_mask = torch.LongTensor([attn]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        u, v = batch_uv_embeddings(model, input_ids, attention_mask)\n",
    "        logits = classifier_head(u, v)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return label_map[pred]\n",
    "\n",
    "# test\n",
    "print(predict_nli(\"A man is playing a guitar on stage.\", \"The man is performing music.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c09fa",
   "metadata": {},
   "source": [
    "Task 3. Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06eb1b",
   "metadata": {},
   "source": [
    "Evaluate on TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8dfde4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 63/63 [00:00<00:00, 143.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4095\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.45      0.43      0.44       690\n",
      "      neutral       0.39      0.61      0.47       660\n",
      "contradiction       0.40      0.18      0.25       650\n",
      "\n",
      "     accuracy                           0.41      2000\n",
      "    macro avg       0.41      0.41      0.39      2000\n",
      " weighted avg       0.41      0.41      0.39      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "classifier_head.eval()\n",
    "\n",
    "preds_all, labels_all = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Test\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        u, v = batch_uv_embeddings(model, input_ids, attention_mask)\n",
    "        logits = classifier_head(u, v)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        preds_all.extend(preds.cpu().numpy().tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "target_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "print(\"Test accuracy:\", accuracy_score(labels_all, preds_all))\n",
    "print(classification_report(labels_all, preds_all, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f8501",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f894d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[298 307  85]\n",
      " [163 404  93]\n",
      " [196 337 117]]\n",
      "\n",
      "Normalized confusion matrix:\n",
      " [[0.432 0.445 0.123]\n",
      " [0.247 0.612 0.141]\n",
      " [0.302 0.518 0.18 ]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(labels_all, preds_all)\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Optional: normalize to percentages\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "print(\"\\nNormalized confusion matrix:\\n\", np.round(cm_norm, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ed0be",
   "metadata": {},
   "source": [
    "The model achieved a test accuracy of approximately 41%, which is above the random baseline of 33% for three classes, indicating that the learned sentence embeddings capture meaningful semantic information. Performance is strongest on the neutral class, while contradiction has significantly lower recall. The confusion matrix shows that many contradiction and entailment examples are misclassified as neutral, suggesting that the learned embeddings are not yet sufficiently discriminative to clearly separate fine-grained semantic relationships.\n",
    "\n",
    "I use some limit choices on hyperparameters to ensure efficient runtime. The BERT encoder was trained with architecture (2 layers, hidden size 128) and a maximum sequence length of 64 to allow CPU-based training. Additionally, only a subset of 20,000 SNLI samples and one training epoch were used for fine-tuning. While these decisions improved computational efficiency, they likely limited the model’s representational capacity and led to underfitting. Increasing model size, training duration, sequence length, and adopting a subword tokenizer would likely improve contradiction detection and overall performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
